
@inproceedings{wu_lightwrite_2021,
	address = {Yokohama Japan},
	title = {{LightWrite}: {Teach} {Handwriting} to {The} {Visually} {Impaired} with {A} {Smartphone}},
	isbn = {978-1-4503-8096-6},
	shorttitle = {{LightWrite}},
	url = {https://dl.acm.org/doi/10.1145/3411764.3445322},
	doi = {10.1145/3411764.3445322},
	abstract = {Learning to write is challenging for blind and low vision (BLV) people because of the lack of visual feedback. Regardless of the drastic advancement of digital technology, handwriting is still an essential part of daily life. Although tools designed for teaching BLV to write exist, many are expensive and require the help of sighted teachers. We propose LightWrite, a low-cost, easy-to-access smartphone application that uses voice-based descriptive instruction and feedback to teach BLV users to write English lowercase letters and Arabian digits in a specifcally designed font. A two-stage study with 15 BLV users with little prior writing knowledge shows that LightWrite can successfully teach users to learn handwriting characters in an average of 1.09 minutes for each letter. After initial training and 20-minute daily practice for 5 days, participants were able to write an average of 19.9 out of 26 letters that are recognizable by sighted raters.},
	language = {en},
	urldate = {2024-05-21},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Wu, Zihan and Yu, Chun and Xu, Xuhai and Wei, Tong and Zou, Tianyuan and Wang, Ruolin and Shi, Yuanchun},
	month = may,
	year = {2021},
	keywords = {已读/1},
	pages = {1--15},
	file = {Wu et al. - 2021 - LightWrite Teach Handwriting to The Visually Impa.pdf:/Volumes/test3/Users/wuyaoyao/Zotero/storage/7GQRRKBW/Wu et al. - 2021 - LightWrite Teach Handwriting to The Visually Impa.pdf:application/pdf},
}

@patent{horvitz_notificationplatform_nodate,
	title = {{NOTIFICATIONPLATFORM} {ARCHITECTURE}},
	author = {Horvitz, Eric},
	file = {US7243130.pdf:/Volumes/test3/Users/wuyaoyao/Zotero/storage/S25KZIYJ/US7243130.pdf:application/pdf},
}

@inproceedings{mack_what_2021,
	address = {Yokohama Japan},
	title = {What {Do} {We} {Mean} by “{Accessibility} {Research}”?: {A} {Literature} {Survey} of {Accessibility} {Papers} in {CHI} and {ASSETS} from 1994 to 2019},
	isbn = {978-1-4503-8096-6},
	shorttitle = {What {Do} {We} {Mean} by “{Accessibility} {Research}”?},
	url = {https://dl.acm.org/doi/10.1145/3411764.3445412},
	doi = {10.1145/3411764.3445412},
	abstract = {Accessibility research has grown substantially in the past few decades, yet there has been no literature review of the field. To understand current and historical trends, we created and analyzed a dataset of accessibility papers appearing at CHI and ASSETS since ASSETS’ founding in 1994. We qualitatively coded areas of focus and methodological decisions for the past 10 years (20102019, N=506 papers), and analyzed paper counts and keywords over the full 26 years (N=836 papers). Our findings highlight areas that have received disproportionate attention and those that are underserved—for example, over 43\% of papers in the past 10 years are on accessibility for blind and low vision people. We also capture common study characteristics, such as the roles of disabled and nondisabled participants as well as sample sizes (e.g., a median of 13 for participant groups with disabilities and older adults). We close by critically reflecting on gaps in the literature and offering guidance for future work in the field.},
	language = {en},
	urldate = {2024-05-21},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Mack, Kelly and McDonnell, Emma and Jain, Dhruv and Lu Wang, Lucy and E. Froehlich, Jon and Findlater, Leah},
	month = may,
	year = {2021},
	pages = {1--18},
	file = {3411764.3445412-2.pdf:/Users/wuyaoyao/Downloads/3411764.3445412-2.pdf:application/pdf},
}

@article{lascau_sometimes_2024,
	title = {“{Sometimes} {It}’s {Like} {Putting} the {Track} in {Front} of the {Rushing} {Train}”: {Having} to {Be} ‘{On} {Call}’ for {Work} {Limits} the {Temporal} {Flexibility} of {Crowdworkers}},
	volume = {31},
	issn = {1073-0516, 1557-7325},
	shorttitle = {“{Sometimes} {It}’s {Like} {Putting} the {Track} in {Front} of the {Rushing} {Train}”},
	url = {https://dl.acm.org/doi/10.1145/3635145},
	doi = {10.1145/3635145},
	abstract = {Research suggests that the temporal flexibility advertised to crowdworkers by crowdsourcing platforms is limited by both client-imposed constraints (e.g., strict completion times) and crowdworkers’ tooling practices (e.g., multitasking). In this article, we explore an additional contributor to workers’ limited temporal flexibility: the design of crowdsourcing platforms, namely requiring crowdworkers to be ‘on call’ for work. We conducted two studies to investigate the impact of having to be ‘on call’ on workers’ schedule control and job control. We find that being ‘on call’ impacted (1) participants’ ability to schedule their time and stick to planned work hours, and (2) the pace at which participants worked and took breaks. The results of the two studies suggest that the ‘on-demand’ nature of crowdsourcing platforms can limit workers’ temporal flexibility by reducing schedule control and job control. We conclude the article by discussing the implications of the results for (a) crowdworkers, (b) crowdsourcing platforms, and (c) the wider platform economy.},
	language = {en},
	number = {2},
	urldate = {2024-05-21},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Lascău, Laura and Brumby, Duncan P. and Gould, Sandy J. J. and Cox, Anna L.},
	month = apr,
	year = {2024},
	pages = {1--45},
	file = {Lascău et al. - 2024 - “Sometimes It’s Like Putting the Track in Front of.pdf:/Volumes/test3/Users/wuyaoyao/Zotero/storage/DUWW9BTF/Lascău et al. - 2024 - “Sometimes It’s Like Putting the Track in Front of.pdf:application/pdf},
}

@article{bi_multilingual_nodate,
	title = {Multilingual {Touchscreen} {Keyboard} {Design} and {Optimization}},
	language = {en},
	author = {Bi, Xiaojun},
	keywords = {已读/3, 好文/3},
	file = {Bi - Multilingual Touchscreen Keyboard Design and Optim.pdf:/Volumes/test3/Users/wuyaoyao/Zotero/storage/962DZQXB/Bi - Multilingual Touchscreen Keyboard Design and Optim.pdf:application/pdf},
}

@article{zhao_data_2008,
	title = {Data {Sonification} for {Users} with {Visual} {Impairment}: {A} {Case} {Study} with {Georeferenced} {Data}},
	volume = {15},
	issn = {1073-0516, 1557-7325},
	shorttitle = {Data {Sonification} for {Users} with {Visual} {Impairment}},
	url = {https://dl.acm.org/doi/10.1145/1352782.1352786},
	doi = {10.1145/1352782.1352786},
	abstract = {We describe the development and evaluation of a tool, iSonic, to assist users with visual impairment in exploring georeferenced data using coordinated maps and tables, augmented with nontextual sounds and speech output. Our in-depth case studies with 7 blind users during 42 hours of data collection, showed that iSonic enabled them to find facts and discover trends in georeferenced data, even in unfamiliar geographical contexts, without special devices. Our design was guided by an Action-by-Design-Component (ADC) framework, which was also applied to scatterplots to demonstrate its generalizability. Video and download is available at www.cs.umd.edu/hcil/iSonic/.},
	language = {en},
	number = {1},
	urldate = {2024-05-21},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Zhao, Haixia and Plaisant, Catherine and Shneiderman, Ben and Lazar, Jonathan},
	month = may,
	year = {2008},
	pages = {1--28},
	file = {Zhao et al. - 2008 - Data Sonification for Users with Visual Impairment.pdf:/Volumes/test3/Users/wuyaoyao/Zotero/storage/JRISJX2I/Zhao et al. - 2008 - Data Sonification for Users with Visual Impairment.pdf:application/pdf},
}

@inproceedings{zhai_movement_2002,
	address = {Minneapolis Minnesota USA},
	title = {Movement model, hits distribution and learning in virtual keyboarding},
	isbn = {978-1-58113-453-7},
	url = {https://dl.acm.org/doi/10.1145/503376.503381},
	doi = {10.1145/503376.503381},
	abstract = {In a ten-session experiment, six participants practiced typing with an expanding rehearsal method on an optimized virtual keyboard. Based on a large amount of in-situ performance data, this paper reports the following findings. First, the Fitts-digraph movement efficiency model of virtual keyboards is revised. The format and parameters of Fitts’ law used previously in virtual keyboards research were incorrect. Second, performance limit predictions of various layouts are calculated with the new model. Third, learning with expanding rehearsal intervals for maximum memory benefits is effective, but many improvements of the training algorithm used can be made in the future. Finally, increased visual load when typing previously practiced text did not significantly change users’ performance at this stage of learning, but typing unpracticed text did have a performance effect, suggesting a certain degree of text specific learning when typing on virtual keyboards.},
	language = {en},
	urldate = {2024-05-21},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Zhai, Shumin and Sue, Alison and Accot, Johnny},
	month = apr,
	year = {2002},
	pages = {17--24},
	file = {Zhai et al. - 2002 - Movement model, hits distribution and learning in .pdf:/Volumes/test3/Users/wuyaoyao/Zotero/storage/NH5SCF6Q/Zhai et al. - 2002 - Movement model, hits distribution and learning in .pdf:application/pdf},
}

@inproceedings{khanna_hand_2024,
	address = {Honolulu HI USA},
	title = {Hand {Gesture} {Recognition} for {Blind} {Users} by {Tracking} {3D} {Gesture} {Trajectory}},
	isbn = {9798400703300},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642602},
	doi = {10.1145/3613904.3642602},
	abstract = {Hand gestures provide an alternate interaction modality for blind users and can be supported using commodity smartwatches without requiring specialized sensors. The enabling technology is an accurate gesture recognition algorithm, but almost all algorithms are designed for sighted users. Our study shows that blind user gestures are considerably different from sighted users, rendering current recognition algorithms unsuitable. Blind user gestures have high inter-user variance, making learning gesture patterns difficult without large-scale training data. Instead, we design a gesture recognition algorithm that works on a 3D representation of the gesture trajectory, capturing motion in free space. Our insight is to extract a micro-movement in the gesture that is user-invariant and use this micro-movement for gesture classification. To this end, we develop an ensemble classifier that combines image classification with geometric properties of the gesture. Our evaluation demonstrates a 92\% classification accuracy, surpassing the next best state-of-the-art which has an accuracy of 82\%.},
	language = {en},
	urldate = {2024-05-21},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Khanna, Prerna and Ramakrishnan, Iv and Jain, Shubham and Bi, Xiaojun and Balasubramanian, Aruna},
	month = may,
	year = {2024},
	pages = {1--15},
	file = {Khanna et al. - 2024 - Hand Gesture Recognition for Blind Users by Tracki.pdf:/Volumes/test3/Users/wuyaoyao/Zotero/storage/IREWLI78/Khanna et al. - 2024 - Hand Gesture Recognition for Blind Users by Tracki.pdf:application/pdf},
}

@article{lu_teaching_2020,
	title = {Teaching and {Learning} {Chinese} through {Immersion}: {A} {Case} {Study} from the {North} {American} {Context}},
	volume = {15},
	issn = {1673-341X, 1673-3533},
	shorttitle = {Teaching and {Learning} {Chinese} through {Immersion}},
	url = {http://link.springer.com/10.1007/s11516-020-0005-9},
	doi = {10.1007/s11516-020-0005-9},
	abstract = {With the promise of achieving bilingualism, biliteracy, and cultural pluralism, Chinese immersion programs for students from kindergarten to 12th grade (K-12) in North America, especially the US, have been proliferating in the past two decades. Research on this rapidly growing population of non-native Chinese learners is also growing. This research synthesis focuses on 35 selected studies published in recent years on Chinese immersion in both Chinese and English language journals and books. The review found that researchers are exploring a wide range of issues with respect to language and literacy development in Chinese immersion programs, including academic achievement in English, language and literacy acquisition in Chinese, instructional strategies and classroom interaction, as well as learners’ language use and its sociolinguistic variations. These studies reflect a growing interest in and demand for learning more about the lesser-researched Chinese foreign language (CFL) learner population, and this review concludes with suggestions for future research on Chinese immersion based on its curricular features as well as specific considerations for conducting research with young, emergent bilingual and biliterate learners.},
	language = {en},
	number = {1},
	urldate = {2024-05-21},
	journal = {Frontiers of Education in China},
	author = {Lü, Chan},
	month = mar,
	year = {2020},
	pages = {99--141},
	file = {Lü - 2020 - Teaching and Learning Chinese through Immersion A.pdf:/Volumes/test3/Users/wuyaoyao/Zotero/storage/6RBJT4CS/Lü - 2020 - Teaching and Learning Chinese through Immersion A.pdf:application/pdf},
}

@book{zhang_crossing_2023,
	address = {Cham},
	series = {Educational {Linguistics}},
	title = {Crossing {Boundaries} in {Researching}, {Understanding}, and {Improving} {Language} {Education}: {Essays} in {Honor} of {G}. {Richard} {Tucker}},
	volume = {58},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-031-24077-5 978-3-031-24078-2},
	shorttitle = {Crossing {Boundaries} in {Researching}, {Understanding}, and {Improving} {Language} {Education}},
	url = {https://link.springer.com/10.1007/978-3-031-24078-2},
	language = {en},
	urldate = {2024-05-21},
	publisher = {Springer International Publishing},
	editor = {Zhang, Dongbo and Miller, Ryan T.},
	year = {2023},
	doi = {10.1007/978-3-031-24078-2},
	file = {Zhang and Miller - 2023 - Crossing Boundaries in Researching, Understanding,.pdf:/Volumes/test3/Users/wuyaoyao/Zotero/storage/X9M8XZ6M/Zhang and Miller - 2023 - Crossing Boundaries in Researching, Understanding,.pdf:application/pdf},
}

@article{ling_model_2024,
	title = {Model {Touch} {Pointing} and {Detect} {Parkinson}'s {Disease} via a {Mobile} {Game}},
	volume = {8},
	issn = {2474-9567},
	url = {https://dl.acm.org/doi/10.1145/3659627},
	doi = {10.1145/3659627},
	abstract = {Touch pointing is one of the primary interaction actions on mobile devices. In this research, we aim to (1) model touch pointing for people with Parkinson's Disease (PD), and (2) detect PD via touch pointing. We created a mobile game called MoleBuster in which a user performs a sequence of pointing actions. Our study with 40 participants shows that PD participants exhibited distinct pointing behavior. PD participants were much slower and had greater variances in movement time (MT), while their error rate was slightly lower than age-matched non-PD participants, indicating PD participants traded speed for accuracy. The nominal width Finger-Fitts law showed greater fitness than Fitts' law, suggesting this model should be adopted in lieu of Fitts' law to guide mobile interface design for PD users. We also proposed a CNN-Transformer-based neural network model to detect PD. Taking touch pointing data and comfort rating of finger movement as input, this model achieved an AUC of 0.97 and sensitivity of 0.95 in leave-one-user-out cross-validation. Overall, our research contributes models that reveal the temporal and spatial characteristics of touch pointing for PD users, and provide a new method (CNN-Transformer model) and a mobile game (MoleBuster) for convenient PD detection.},
	language = {en},
	number = {2},
	urldate = {2024-05-21},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Ling, Kaiyan and Zhao, Hang and Fan, Xiangmin and Niu, Xiaohui and Yin, Wenchao and Liu, Yue and Wang, Cui and Bi, Xiaojun},
	month = may,
	year = {2024},
	pages = {1--24},
	file = {Ling et al. - 2024 - Model Touch Pointing and Detect Parkinson's Diseas.pdf:/Volumes/test3/Users/wuyaoyao/Zotero/storage/W3EIY5ZT/Ling et al. - 2024 - Model Touch Pointing and Detect Parkinson's Diseas.pdf:application/pdf},
}

@inproceedings{sharif_unlockedmaps_2022,
	address = {Athens Greece},
	title = {{UnlockedMaps}: {Visualizing} {Real}-{Time} {Accessibility} of {Urban} {Rail} {Transit} {Using} a {Web}-{Based} {Map}},
	isbn = {978-1-4503-9258-7},
	shorttitle = {{UnlockedMaps}},
	url = {https://dl.acm.org/doi/10.1145/3517428.3550397},
	doi = {10.1145/3517428.3550397},
	abstract = {Current web-based maps do not provide visibility into real-time elevator outages at urban rail transit stations, disenfranchising commuters (e.g., wheelchair users) who rely on functioning elevators at transit stations. In this paper, we demonstrate UnlockedMaps, an open-source and open-data web-based map that visualizes the real-time accessibility of urban rail transit stations in six North American cities, assisting users in making informed decisions regarding their commute. Specifcally, UnlockedMaps uses a map to display transit stations, prominently highlighting their real-time accessibility status (accessible with functioning elevators, accessible but experiencing at least one elevator outage, or not-accessible) and surrounding accessible restaurants and restrooms. UnlockedMaps is the frst system to collect elevator outage data from 2,336 transit stations over 23 months and make it publicly available via an API. We report on results from our pilot user studies with fve stakeholder groups: (1) people with mobility disabilities; (2) pregnant people; (3) cyclists/stroller users/commuters with heavy equipment; (4) members of disability advocacy groups; and (5) civic hackers.},
	language = {en},
	urldate = {2024-05-21},
	booktitle = {Proceedings of the 24th {International} {ACM} {SIGACCESS} {Conference} on {Computers} and {Accessibility}},
	publisher = {ACM},
	author = {Sharif, Ather and Ramesh, Aneesha and Nguyen, Trung-Anh and Chen, Luna and Zeng, Kent Richard and Hou, Lanqing and Xu, Xuhai},
	month = oct,
	year = {2022},
	keywords = {已读/3, 可参考/水},
	pages = {1--7},
	file = {Sharif et al. - 2022 - UnlockedMaps Visualizing Real-Time Accessibility .pdf:/Volumes/test3/Users/wuyaoyao/Zotero/storage/RZICVGAN/Sharif et al. - 2022 - UnlockedMaps Visualizing Real-Time Accessibility .pdf:application/pdf},
}

@inproceedings{xu_earbuddy_2020,
	address = {Honolulu HI USA},
	title = {{EarBuddy}: {Enabling} {On}-{Face} {Interaction} via {Wireless} {Earbuds}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {{EarBuddy}},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376836},
	doi = {10.1145/3313831.3376836},
	abstract = {Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classiﬁcation. Our optimized classiﬁer achieved an accuracy of 95.3\%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability.},
	language = {en},
	urldate = {2024-05-21},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.},
	month = apr,
	year = {2020},
	pages = {1--14},
	file = {Xu et al. - 2020 - EarBuddy Enabling On-Face Interaction via Wireles.pdf:/Volumes/test3/Users/wuyaoyao/Zotero/storage/6ACREA9C/Xu et al. - 2020 - EarBuddy Enabling On-Face Interaction via Wireles.pdf:application/pdf},
}

@inproceedings{xu_hulamove_2021,
	address = {Yokohama Japan},
	title = {{HulaMove}: {Using} {Commodity} {IMU} for {Waist} {Interaction}},
	isbn = {978-1-4503-8096-6},
	shorttitle = {{HulaMove}},
	url = {https://dl.acm.org/doi/10.1145/3411764.3445182},
	doi = {10.1145/3411764.3445182},
	abstract = {We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We frst conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confrm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5\%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove signifcantly reduced interaction time by 41.8\% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.},
	language = {en},
	urldate = {2024-05-21},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Xu, Xuhai and Li, Jiahao and Yuan, Tianyi and He, Liang and Liu, Xin and Yan, Yukang and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K},
	month = may,
	year = {2021},
	pages = {1--16},
	file = {Xu et al. - 2021 - HulaMove Using Commodity IMU for Waist Interactio.pdf:/Volumes/test3/Users/wuyaoyao/Zotero/storage/EWIR27JD/Xu et al. - 2021 - HulaMove Using Commodity IMU for Waist Interactio.pdf:application/pdf},
}

@inproceedings{xu_enabling_2022,
	address = {New Orleans LA USA},
	title = {Enabling {Hand} {Gesture} {Customization} on {Wrist}-{Worn} {Devices}},
	isbn = {978-1-4503-9157-3},
	url = {https://dl.acm.org/doi/10.1145/3491102.3501904},
	doi = {10.1145/3491102.3501904},
	abstract = {We present a framework for gesture customization requiring minimal examples from users, all without degrading the performance of existing gesture sets. To achieve this, we frst deployed a large-scale study (N=500+) to collect data and train an accelerometer-gyroscope recognition model with a cross-user accuracy of 95.7\% and a falsepositive rate of 0.6 per hour when tested on everyday non-gesture data. Next, we design a few-shot learning framework which derives a lightweight model from our pre-trained model, enabling knowledge transfer without performance degradation. We validate our approach through a user study (N=20) examining on-device customization from 12 new gestures, resulting in an average accuracy of 55.3\%, 83.1\%, and 87.2\% on using one, three, or fve shots when adding a new gesture, while maintaining the same recognition accuracy and false-positive rate from the pre-existing gesture set. We further evaluate the usability of our real-time implementation with a user experience study (N=20). Our results highlight the efectiveness, learnability, and usability of our customization framework.},
	language = {en},
	urldate = {2024-05-21},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Xu, Xuhai and Gong, Jun and Brum, Carolina and Liang, Lilian and Suh, Bongsoo and Gupta, Shivam Kumar and Agarwal, Yash and Lindsey, Laurence and Kang, Runchang and Shahsavari, Behrooz and Nguyen, Tu and Nieto, Heriberto and Hudson, Scott E and Maalouf, Charlie and Mousavi, Jax Seyed and Laput, Gierad},
	month = apr,
	year = {2022},
	pages = {1--19},
	file = {Xu et al. - 2022 - Enabling Hand Gesture Customization on Wrist-Worn .pdf:/Volumes/test3/Users/wuyaoyao/Zotero/storage/8UCEE47D/Xu et al. - 2022 - Enabling Hand Gesture Customization on Wrist-Worn .pdf:application/pdf},
}

@inproceedings{xu_typeout_2022,
	address = {New Orleans LA USA},
	title = {{TypeOut}: {Leveraging} {Just}-in-{Time} {Self}-{Affirmation} for {Smartphone} {Overuse} {Reduction}},
	copyright = {http://www.acm.org/publications/policies/copyright\_policy\#Background},
	isbn = {978-1-4503-9157-3},
	shorttitle = {{TypeOut}},
	url = {https://dl.acm.org/doi/10.1145/3491102.3517476},
	doi = {10.1145/3491102.3517476},
	abstract = {Smartphone overuse is related to a variety of issues such as lack of sleep and anxiety. We explore the application of Self-Afrmation Theory on smartphone overuse intervention in a just-in-time manner. We present TypeOut, a just-in-time intervention technique that integrates two components: an in-situ typing-based unlock process to improve user engagement, and self-afrmation-based typing content to enhance efectiveness. We hypothesize that the integration of typing and self-afrmation content can better reduce smartphone overuse. We conducted a 10-week within-subject feld experiment (N=54) and compared TypeOut against two baselines: one only showing the self-afrmation content (a common notifcation-based intervention), and one only requiring typing non-semantic content (a state-of-the-art method). TypeOut reduces app usage by over 50\%, and both app opening frequency and usage duration by over 25\%, all signifcantly outperforming baselines. TypeOut can potentially be used in other domains where an intervention may beneft from integrating self-afrmation exercises with an engaging just-in-time mechanism.},
	language = {en},
	urldate = {2024-05-21},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Xu, Xuhai and Zou, Tianyuan and Xiao, Han and Li, Yanzhang and Wang, Ruolin and Yuan, Tianyi and Wang, Yuntao and Shi, Yuanchun and Mankoff, Jennifer and Dey, Anind K},
	month = apr,
	year = {2022},
	pages = {1--17},
	file = {Xu et al. - 2022 - TypeOut Leveraging Just-in-Time Self-Affirmation .pdf:/Volumes/test3/Users/wuyaoyao/Zotero/storage/NQV8HKV8/Xu et al. - 2022 - TypeOut Leveraging Just-in-Time Self-Affirmation .pdf:application/pdf},
}

@inproceedings{horvitz_principles_1999,
	address = {Pittsburgh, Pennsylvania, United States},
	title = {Principles of mixed-initiative user interfaces},
	isbn = {978-0-201-48559-2},
	url = {http://portal.acm.org/citation.cfm?doid=302979.303030},
	doi = {10.1145/302979.303030},
	abstract = {Recent debate has centered on the relative promise of focusing user-interface research on developing new metaphors and tools that enhance users’ abilities to directly manipulate objects versus directing effort toward developing interface agents that provide automation. In this paper, we review principles that show promise for allowing engineers to enhance human-computer interaction through an elegant coupling of automated services with direct manipulation. Key ideas will be highlighted in terms of the Lookout system for scheduling and meeting management.},
	language = {en},
	urldate = {2024-05-21},
	booktitle = {Proceedings of the {SIGCHI} conference on {Human} factors in computing systems the {CHI} is the limit - {CHI} '99},
	publisher = {ACM Press},
	author = {Horvitz, Eric},
	year = {1999},
	pages = {159--166},
	file = {Horvitz - 1999 - Principles of mixed-initiative user interfaces.pdf:/Volumes/test3/Users/wuyaoyao/Zotero/storage/SSXT4E9R/Horvitz - 1999 - Principles of mixed-initiative user interfaces.pdf:application/pdf},
}

@inproceedings{amershi_guidelines_2019,
	address = {Glasgow Scotland Uk},
	title = {Guidelines for {Human}-{AI} {Interaction}},
	isbn = {978-1-4503-5970-2},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300233},
	doi = {10.1145/3290605.3300233},
	abstract = {Advances in artifcial intelligence (AI) frame opportunities and challenges for user interface design. Principles for humanAI interaction have been discussed in the human-computer interaction community for over two decades, but more study and innovation are needed in light of advances in AI and the growing uses of AI technologies in human-facing applications. We propose 18 generally applicable design guidelines for human-AI interaction. These guidelines are validated through multiple rounds of evaluation including a user study with 49 design practitioners who tested the guidelines against 20 popular AI-infused products. The results verify the relevance of the guidelines over a spectrum of interaction scenarios and reveal gaps in our knowledge, highlighting opportunities for further research. Based on the evaluations, we believe the set of design guidelines can serve as a resource to practitioners working on the design of applications and features that harness AI technologies, and to researchers interested in the further development of guidelines for human-AI interaction design.},
	language = {en},
	urldate = {2024-05-21},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Amershi, Saleema and Weld, Dan and Vorvoreanu, Mihaela and Fourney, Adam and Nushi, Besmira and Collisson, Penny and Suh, Jina and Iqbal, Shamsi and Bennett, Paul N. and Inkpen, Kori and Teevan, Jaime and Kikin-Gil, Ruth and Horvitz, Eric},
	month = may,
	year = {2019},
	pages = {1--13},
	file = {Amershi et al. - 2019 - Guidelines for Human-AI Interaction.pdf:/Volumes/test3/Users/wuyaoyao/Zotero/storage/58HJX239/Amershi et al. - 2019 - Guidelines for Human-AI Interaction.pdf:application/pdf},
}

@inproceedings{czerwinski_diary_2004,
	address = {Vienna Austria},
	title = {A diary study of task switching and interruptions},
	isbn = {978-1-58113-702-6},
	url = {https://dl.acm.org/doi/10.1145/985692.985715},
	doi = {10.1145/985692.985715},
	abstract = {We report on a diary study of the activities of information workers aimed at characterizing how people interleave multiple tasks amidst interruptions. The week-long study revealed the type and complexity of activities performed, the nature of the interruptions experienced, and the difficulty of shifting among numerous tasks. We present key findings from the diary study and discuss implications of the findings. Finally, we describe promising directions in the design of software tools for task management, motivated by the findings.},
	language = {en},
	urldate = {2024-05-21},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Czerwinski, Mary and Horvitz, Eric and Wilhite, Susan},
	month = apr,
	year = {2004},
	pages = {175--182},
	file = {Czerwinski et al. - 2004 - A diary study of task switching and interruptions.pdf:/Volumes/test3/Users/wuyaoyao/Zotero/storage/HRMFVLCT/Czerwinski et al. - 2004 - A diary study of task switching and interruptions.pdf:application/pdf},
}
